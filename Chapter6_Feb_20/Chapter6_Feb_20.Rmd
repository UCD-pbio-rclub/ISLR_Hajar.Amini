---
title: "Chapter6_Feb_20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#6.8 Exercises
Conceptual
1. We perform best subset, forward stepwise, and backward stepwise
selection on a single data set. For each approach, we obtain p + 1
models, containing 0, 1, 2, . . . , p predictors. Explain your answers:
(a) Which of the three models with k predictors has the smallest
training RSS?
Best subset selection has the smallest training RSS because the other two methods determine models based on which predictors they pick first. 

(b) Which of the three models with k predictors has the smallest
test RSS?
Best subset selection have the smallest test RSS because it considers more models than the other methods, but the other models might can pick more model that fits the test data better.

(c) True or False:
i. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by forward stepwise selection.
True
ii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by backward stepwise selection.
True
iii. The predictors in the k-variable model identified by backward
stepwise are a subset of the predictors in the (k + 1)-
variable model identified by forward stepwise selection.
False
iv. The predictors in the k-variable model identified by forward
stepwise are a subset of the predictors in the (k+1)-variable
model identified by backward stepwise selection.
False
v. The predictors in the k-variable model identified by best
subset are a subset of the predictors in the (k + 1)-variable
model identified by best subset selection.
False

8. In this exercise, we will generate simulated data, and will then use
this data to perform best subset selection.

(a) Use the rnorm() function to generate a predictor X of length
n = 100, as well as a noise vector e of length n = 100.

```{r}
set.seed(1)
X <- rnorm(100)
e <- rnorm(100)
```

(b) Generate a response vector Y of length n = 100 according to
the model
Y = β0 + β1X + β2X2 + β3X3 + e,
where β0, β1, β2, and β3 are constants of your choice.

```{r}
β0 = 3 

β1 = 2 
β2 = -3 
β3=0.3
Y = β0 + β1 * X + β2 * X^2 + β3 * X^3 + e
```

(c) Use the regsubsets() function to perform best subset selection
in order to choose the best model containing the predictors
X,X2, . . .,X10. What is the best model obtained according to
Cp, BIC, and adjusted R2? Show some plots to provide evidence
for your answer, and report the coefficients of the best model obtained.
Note you will need to use the data.frame() function to
create a single data set containing both X and Y .

```{r}
#best modle having polynomial of X of degree 10
#install.packages("leaps")
library(leaps)
data <- data.frame("y" = Y, "x" = X)
mode <- regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10)
mode.summary = summary(mode)
# Find the model size for best cp, BIC and adjr2
which.min(mode.summary$cp) #[1] 3
which.min(mode.summary$bic) #[1] 3
which.max(mode.summary$adjr2) #[1] 5
# Plot cp, BIC and adjr2
plot(mode.summary$cp, xlab="Subset degree", ylab="Cp", pch=20, type="l")
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
plot(mode.summary$bic, xlab="Subset degree", ylab="BIC", pch=20, type="l")
points(3, mode.summary$bic[3], pch=4, col="red", lwd=7)
plot(mode.summary$adjr2, xlab="Subset degree", ylab="Adjusted R2", pch=20, type="l")
points(3, mode.summary$adjr2[3], pch=4, col="red", lwd=7)
#We find that with Cp, BIC and Adjusted R2 criteria, 3, 3, and 3 variable models are respectively picked.
coefficients(mode, id=3)
#All statistics pick $X^7$ over $X^3$. The remaining coefficients are quite close to $\beta$ s.
```

(d) Repeat (c), using forward stepwise selection and also using backwards
stepwise selection. How does your answer compare to the
results in (c)?

```{r}
#We fit forward and backward stepwise models to the data.
mode.fwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="forward")
mode.bwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="backward")
fwd.summary = summary(mode.fwd)
bwd.summary = summary(mode.bwd)
which.min(fwd.summary$cp)
which.min(bwd.summary$cp)
which.min(fwd.summary$bic)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
which.max(bwd.summary$adjr2)
# Plot the statistics
par(mfrow=c(3, 2))
plot(fwd.summary$cp, xlab="Subset Size", ylab="Forward Cp", pch=20, type="l")
points(3, fwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(bwd.summary$cp, xlab="Subset Size", ylab="Backward Cp", pch=20, type="l")
points(3, bwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(fwd.summary$bic, xlab="Subset Size", ylab="Forward BIC", pch=20, type="l")
points(3, fwd.summary$bic[3], pch=4, col="red", lwd=7)
plot(bwd.summary$bic, xlab="Subset Size", ylab="Backward BIC", pch=20, type="l")
points(3, bwd.summary$bic[3], pch=4, col="red", lwd=7)
plot(fwd.summary$adjr2, xlab="Subset Size", ylab="Forward Adjusted R2", pch=20, type="l")
points(3, fwd.summary$adjr2[3], pch=4, col="red", lwd=7)
plot(bwd.summary$adjr2, xlab="Subset Size", ylab="Backward Adjusted R2", pch=20, type="l")
points(4, bwd.summary$adjr2[4], pch=4, col="red", lwd=7)
#see that all statistics pick 3 variable models except backward stepwise with adjusted R2. Here are the coefficients

coefficients(mode.fwd, id=3)
coefficients(mode.bwd, id=3)
coefficients(mode.fwd, id=4)
#Here forward stepwise picks $X^7$ over $X^3$. Backward stepwise with $3$ variables picks $X^9$ while backward stepwise with $4$ variables picks $X^4$ and $X^7$. All other coefficients are close to $\beta$ s.
```

(e) Now fit a lasso model to the simulated data, again using X,X2,
. . . , X10 as predictors. Use cross-validation to select the optimal
value of λ. Create plots of the cross-validation error as a function
of λ. Report the resulting coefficient estimates, and discuss the
results obtained.
(f) Now generate a response vector Y according to the model
Y = β0 + β7X7 + ,
and perform best subset selection and the lasso. Discuss the
results obtained.