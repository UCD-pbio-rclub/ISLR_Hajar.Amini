{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Chapter6_Feb_27\"\noutput: \n  html_document: \n    keep_md: yes\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n#problem\n\n3. Suppose we estimate the regression coefficients in a linear regression\nmodel by minimizing\n\nfor a particular value of s. For parts (a) through (e), indicate which\nof i. through v. is correct. Justify your answer.\n(a) As we increase s from 0, the training RSS will:\ni. Increase initially, and then eventually start decreasing in an\ninverted U shape.\nii. Decrease initially, and then eventually start increasing in a\nU shape.\niii. Steadily increase.\niv. Steadily decrease.\nv. Remain constant.\n\na\n(iv) Steadily decreases is correct: \nMy reason:\nAs we increase s from 0, all beta 's increase from 0 to their least square estimate values. Training error for \"0\" beta s is the maximum and it steadily decreases to the ordinary Least Square RSS\n\n(b) Repeat (a) for test RSS.\nb\n(ii) Decrease initially is correct, \nafter decreasing initially then eventually start increasing in a U shape for this reason:\nWhen s = 0, all beta s are 0, the model has a high test RSS.\nAs we increase s, beta s assume non-zero values and model starts fitting well on test data and so test RSS decreases.\nafter overfitting to the training data, increasing test RSS.\n\n(c) Repeat (a) for variance.\n(iii) Steadily increase is correct: \nWhen s = 0, the model effectively predicts a constant and has almost no variance. As we increase s, the models includes more beta s and their values start increasing. At this point, the beta s become highly dependent on training data, thus increasing the variance.\n\n(d) Repeat (a) for (squared) bias.\n(iv) Steadily decrease is correct:\nWhen s = 0, the model effectively predicts a constant and hence the prediction is far from actual value. Thus bias is high. As s increases, more beta s become non-zero and thus the model continues to fit training data better. And thus, bias decreases.\n\n(e) Repeat (a) for the irreducible error.\n(v) Remains constant is correct: By definition, \nirreducible error is model independent and hence s is not inportant, in all situation remains constant.\n\n4. Suppose we estimate the regression coefficients in a linear regression\nmodel by minimizing\n\nfor a particular value of λ. For parts (a) through (e), indicate which\nof i. through v. is correct. Justify your answer.\n(a) As we increase λ from 0, the training RSS will:\ni. Increase initially, and then eventually start decreasing in an\ninverted U shape.\nii. Decrease initially, and then eventually start increasing in a\nU shape.\niii. Steadily increase.\niv. Steadily decrease.\nv. Remain constant.\n\n(a)\n(iii) Steadily increase is correct:\nAs we increase lambda from zreo, all beta's decrease from their least square estimate values to zero. Training error is becoming the minimum and it steadily increases as beta s are reduced to zero.\n\n(b) Repeat (a) for test RSS.\n(ii) Decrease initially is correct,\nand then eventually start increasing in a U shape: When lambda = 0, all beta s have their least square estimate values. In this case, the model tries to fit hard to training data and hence test RSS is high. As we increase lambda, beta s start reducing to zero and some of the overfitting is reduced. Thus, test RSS initially decreases. as beta s approach zero, test RSS increases.\n\n(c) Repeat (a) for variance.\n(iv) Steadily decreases: When $\\lambda = 0$, the $\\beta$ s have their least square estimate values. The actual estimates heavily depend on the training data and hence variance is high. As we increase $\\lambda$, $\\beta$ s start decreasing and model becomes simpler. In the limiting case of $\\lambda$ approaching infinity, all $beta$ s reduce to zero and model predicts a constant and has no variance.\n\n(d) Repeat (a) for (squared) bias.\n(iii) Steadily increases is correct: \nWhen lambda = 0,beta s have their least-square estimate values and hence have the least bias. \nAs lambda increases, beta s start reducing towards zero, the model fits less. if limiting case of lambda is close to infinity, the model predicts a constant and hence bias is maximum.\n\n(e) Repeat (a) for the irreducible error.\n(v)Remains constant is correct:\nirreducible error is model independent so without considering lambda, remains constant.\n\n5. It is well-known that ridge regression tends to give similar coefficient\nvalues to correlated variables, whereas the lasso may give quite different\ncoefficient values to correlated variables. We will now explore\nthis property in a very simple setting.\nSuppose that n = 2, p = 2, x11 = x12, x21 = x22. Furthermore,\nsuppose that y1+y2 = 0 and x11+x21 = 0 and x12+x22 = 0, so that\nthe estimate for the intercept in a least squares, ridge regression, or\nlasso model is zero: ˆβ0 = 0.\n\n(a) Write out the ridge regression optimization problem in this setting.\nA general form of Ridge regression optimization looks like\nwrite in notebook\n\n(b) Argue that in this setting, the ridge coefficient estimates satisfy\n\n\n\n9. In this exercise, we will predict the number of applications received\nusing the other variables in the College data set.\n(a) Split the data set into a training set and a test set.\n\n```{r}\nlibrary(ISLR)\nset.seed(11)\nsum(is.na(College))\ntrain.size = dim(College)[1] / 2\ntrain = sample(1:dim(College)[1], train.size)\ntest = -train\nCollege.train = College[train, ]\nCollege.test = College[test, ]\n```\n\n(b) Fit a linear model using least squares on the training set, and\nreport the test error obtained.\n\n```{r}\nlm.fit = lm(Apps~., data=College.train) #fit on training data\nlm.predict = predict(lm.fit, College.test) #predict on test data\nmean((College.test[, \"Apps\"] - lm.predict)^2)\n## [1] 1538442\n#Test RSS is [1] 1538442\n```\n\n(c) Fit a ridge regression model on the training set, with λ chosen\nby cross-validation. Report the test error obtained.\n\n```{r}\nlibrary(glmnet)\ntrain = model.matrix(Apps~., data=College.train)\ntest = model.matrix(Apps~., data=College.test)\ngrid = 10 ^ seq(4, -2, length=100)\nridge.mod = cv.glmnet(train, College.train[, \"Apps\"], alpha=0, lambda=grid, thresh=1e-10) #By default, the function cv.glmnet() performs ten-fold cross-validation, though this can be changed using the argument nfolds\nbestlam = ridge.mod$lambda.min\nbestlam #8.11\nridge.pred = predict(ridge.mod, newx=test, s=bestlam)\nmean((College.test[, \"Apps\"] - ridge.pred)^2)\n## [1] 1568252, Test RSS is slightly higher that OLS.\n\n```\n\n",
    "created" : 1519667300035.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "79593690",
    "id" : "91526456",
    "lastKnownWriteTime" : 1519754889,
    "last_content_update" : 1519754898967,
    "path" : "~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter6_Feb_27/Chapter6_Feb_27.Rmd",
    "project_path" : "Chapter6_Feb_27/Chapter6_Feb_27.Rmd",
    "properties" : {
        "last_setup_crc32" : "43BC3185bb338d19",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}