{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Chapter3_Dec_20\"\noutput: \n  html_document: \n    keep_md: yes\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n#3.6.3 Multiple Linear Regression\n\n```{r}\nlibrary (MASS)\nlibrary (ISLR)\nlm.fit <- lm(data = Boston, medv~lstat+age)\nsummary(lm.fit)\n```\n#for see all variables\n\n```{r}\nlm.fit <- lm(medv~. , data=Boston)\nsummary(lm.fit)\nsummary(lm.fit)$r.sq\n#summary(lm.fit)$sigma gives us the RSE\nsummary(lm.fit)$sigma\n```\n\n```{r}\n#install.packages(\"car\")\nlibrary (car)\nvif(lm.fit)\n#all variables except age\n\nlm.fit1=lm(medv~.-age, data = Boston)\nsummary (lm.fit1)\nlm.fit1=update(lm.fit, ~. -age)\n```\n\n#3.6.4 Interaction Terms\n\n#3.6.5 Non-linear Transformations of the Predictors\n\n```{r}\nlm.fit2=lm(medv~lstat +I(lstat^2), data = Boston)\nsummary(lm.fit2)\nlm.fit=lm(medv~lstat, data=Boston)\nanova(lm.fit ,lm.fit2) #we can see the lm.fit2 fitted better based on F test\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n#for checking the polynominal\nlm.fit5=lm(medv~ poly(lstat, 5), data = Boston) #fifth order fitted better\nsummary(lm.fit5)\nsummary(lm(medv~log(rm), data = Boston))\n```\n\n#3.6.6 Qualitative Predictors\n\n```{r}\nfix( Carseats )\nnames(Carseats )\n```\n\n#problem\n\n2. Carefully explain the differences between the KNN classifier and KNN\nregression methods.\n\nThe result of KNN classifier is the classification output for Y (qualitative), while other output for a KNN regression predicts the\nquantitative value for f(X). both related in formula.\n\n3.Suppose we have a data set with five predictors, X1 =GPA, X2 = IQ,\nX3 = Gender (1 for Female and 0 for Male), X4 = Interaction between\nGPA and IQ, and X5 = Interaction between GPA and Gender. The\nresponse is starting salary after graduation (in thousands of dollars).\nSuppose we use least squares to fit the model, and get ˆβ0 = 50, ˆβ1 =\n20, ˆβ2 = 0.07, ˆβ3 = 35, ˆβ4 = 0.01, ˆβ5 = −10.\n(a) Which answer is correct, and why?\ni. For a fixed value of IQ and GPA, males earn more on average\nthan females.\n \nii. For a fixed value of IQ and GPA, females earn more on\naverage than males.\niii. For a fixed value of IQ and GPA, males earn more on average\nthan females provided that the GPA is high enough.\niv. For a fixed value of IQ and GPA, females earn more on\naverage than males provided that the GPA is high enough.\n\nanswer:\nY = 50 + 20(gpa) + 0.07(iq) + 35(gender) + 0.01(gpa * iq) - 10 (gpa * gender)\nsuppose : gpa l1 iq l2\n(a) Y = 50 + 20 l1 + 0.07 l2 + 35 gender + 0.01(l1 * l2) - 10 (l1 * gender)\nmale: (gender = 0)   50 + 20 l1 + 0.07 l2 + 0.01(l1 * l2)\nfemale: (gender = 1) 50 + 20 l1 + 0.07 l2 + 35 + 0.01(l1 * l2) - 10 (l1)\n\niii. Once the GPA is high enough, males earn more on average.\n\n(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.\n\nanswer:\n(b) Y(Gender = 1, IQ = 110, GPA = 4.0)\n= 50 + 20 * 4 + 0.07 * 110 + 35 + 0.01 (4 * 110) - 10 * 4\n= 137.1\n\n(c) True or false: Since the coefficient for the GPA/IQ interaction\nterm is very small, there is very little evidence of an interaction\neffect. Justify your answer.\n\nanswer:\n(c) False. We must examine the p-value of the regression coefficient to\nunderstand whether the interaction term is statistically significant or not.\n\n9. This question involves the use of multiple linear regression on the\nAuto data set\n\n(d) Use the plot() function to produce diagnostic plots of the linear\nregression fit. Comment on any problems you see with the fit.\nDo the residual plots suggest any unusually large outliers? Does\nthe leverage plot identify any observations with unusually high\nleverage?\n\n```{r}\nlibrary (MASS)\nlibrary (ISLR)\ndata(Auto)\nlm.fit1 = lm(mpg~.-name, data=Auto)\nsummary(lm.fit1)\npar(mfrow=c(2,2))\nplot(lm.fit1)\n\n#The fit does not appear to be accurate because there is an obvious curve pattern to the residuals plots. From the leverage plot, point 14 appears to have high leverage, without a high magnitude residual.\n\nplot(predict(lm.fit1), rstudent(lm.fit1))\n\n#There are possible outliers regarding  plot of studentized residuals because there are data with a value greater than 3.\n```\n\n(e) Use the * and : symbols to fit linear regression models with\ninteraction effects. Do any interactions appear to be statistically\nsignificant?\n\n```{r}\nlm.fit2 = lm(mpg~cylinders*displacement+displacement*weight, data = Auto)\nsummary(lm.fit2)\n\n#picked two highest correlation pairs from correlation matrix.From the p-values, we can see that the interaction between displacement and weight is statistically signifcant, while the interactiion between cylinders and displacement is not.\n```\n\n(f) Try a few different transformations of the variables, such as\nlog(X),√X, X2. Comment on your findings.\n\n```{r}\nlm.fit3 = lm(mpg~log(weight)+sqrt(horsepower)+acceleration+I(acceleration^2), data = Auto)\nsummary(lm.fit3)\npar(mfrow=c(2,2))\nplot(lm.fit3)\nplot(predict(lm.fit3), rstudent(lm.fit3))\n\n#from the p-values, the log(weight), sqrt(horsepower), and acceleration^2 all have statistical significance. The residuals plot has less of a obvious pattern than the plot of all linear regression terms. The studentized residuals displays potential outliers (>3). The leverage plot indicates more than three points with high leverage.\n#The Q-Q plot indicates somewhat unnormality of the residuals. So, a better transformation need to be applied to our model.So in the next attempt, we use log(mpg) as our response variable.\n\n\nlm.fit2<-lm(log(mpg)~cylinders+displacement+horsepower+weight+acceleration+year+origin,data=Auto)\nsummary(lm.fit2)\npar(mfrow=c(2,2)) \nplot(lm.fit2)\nplot(predict(lm.fit2),rstudent(lm.fit2))\n\n#The outputs show that log transform of mpg yield better model fitting regarding better R^2, normality of residuals).\n```\n\n10. This question should be answered using the Carseats data set.\n(h) Is there evidence of outliers or high leverage observations in the\nmodel from (e)?\n\n```{r}\nlibrary(ISLR)\nsummary(Carseats)\nlm.fit2 = lm(Sales ~ Price + US, data = Carseats)\nsummary(lm.fit2)\nplot(predict(lm.fit2), rstudent(lm.fit2))\n\n#All studentized residuals appear to be bounded by -3 to 3, so not outliers \n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n#There are a few observations that greatly exceed $(p+1)/n$ (r 3/397) on the leverage-statistic plot that suggest that the corresponding points have high leverage.\n```\n\n14. This problem focuses on the collinearity problem.\n(a) Perform the following commands in R:\nThe last line corresponds to creating a linear model in which y is\na function of x1 and x2. Write out the form of the linear model.\nWhat are the regression coefficients?\n\n```{r}\nset.seed (1)\nx1=runif (100)\nx2 =0.5* x1+rnorm (100) /10\ny=2+2* x1 +0.3* x2+rnorm (100)\n#Y = 2 + 2 X_1 + 0.3 X_2 + \\epsilon beta_0 = 2, beta_1 = 2, beta_2 = 0.3 \n```\n\n(b) What is the correlation between x1 and x2? Create a scatterplot\ndisplaying the relationship between the variables.\n\n```{r}\ncor(x1, x2)\nplot(x1, x2)\n```\n\n(c) Using this data, fit a least squares regression to predict y using\nx1 and x2. Describe the results obtained. What are ˆ β0, ˆ β1, and\nˆ β2? How do these relate to the true β0, β1, and β2? Can you\nreject the null hypothesis H0 : β1 = 0? How about the null\nhypothesis H0 : β2 = 0?\n\n```{r}\nlm.fit = lm(y~x1+x2)\nsummary(lm.fit)\n#beta_0 = 2.1305 , beta_1 = 1.4396, beta_2 = 1.0097\n# We can reject the null hypothesis for beta_1 because its p-value is below 5%. We cannot reject the null hypothesis for beta_2 because its p-value is much above the 5%\n```\n(d) Now fit a least squares regression to predict y using only x1.\nComment on your results. Can you reject the null hypothesis\nH0 : β1 = 0?\n\n```{r}\nlm.fit = lm(y~x1)\nsummary(lm.fit)\n#Yes, we can reject the null hypothesis for the regression coefficient given the p-value for its t-statistic is near zero.\n```\n\n(e) Now fit a least squares regression to predict y using only x2.\nComment on your results. Can you reject the null hypothesis\nH0 : β1 = 0?\n\n```{r}\nlm.fit = lm(y~x2)\nsummary(lm.fit)\n#Yes, we can reject the null hypothesis for the regression coefficient given the p-value for its t-statistic is near zero.\n```\n(f) Do the results obtained in (c)–(e) contradict each other? Explain\nyour answer.\n\nNo, because x1 and x2 have collinearity, it is hard to distinguish their effects when regressed upon together.\n\n(g) Now suppose we obtain one additional observation, which was\nunfortunately mismeasured.\nRe-fit the linear models from (c) to (e) using this new data. What\neffect does this new observation have on the each of the models?\nIn each model, is this observation an outlier? A high-leverage\npoint? Both? Explain your answers.\n\n```{r}\nx1=c(x1 , 0.1)\nx2=c(x2 , 0.8)\ny=c(y,6)\n\nlm.fit1 = lm(y~x1+x2)\nsummary(lm.fit1)\nlm.fit2 = lm(y~x1)\nsummary(lm.fit2)\nlm.fit3 = lm(y~x2)\nsummary(lm.fit3)\n\n#In the first model, it shifts x1 to statistically insignificance and shifts x2 to statistiscal significance from the change in p-values between the two linear regressions.\n\npar(mfrow=c(2,2))\nplot(lm.fit1)\npar(mfrow=c(2,2))\nplot(lm.fit2)\npar(mfrow=c(2,2))\nplot(lm.fit3)\n#In the first and third models, the point becomes a high leverage point.\n\nplot(predict(lm.fit1), rstudent(lm.fit1))\nplot(predict(lm.fit2), rstudent(lm.fit2))\nplot(predict(lm.fit3), rstudent(lm.fit3))\n\n#Looking at the studentized residuals, we don't observe points too far from the 3 value cutoff, except for the second linear regression: y ~ x1.\n```",
    "created" : 1513716433518.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "782707024",
    "id" : "367D61EF",
    "lastKnownWriteTime" : 1514999745,
    "last_content_update" : 1514999745439,
    "path" : "~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter3_Dec_20/Chapter3_Dec_20.Rmd",
    "project_path" : "Chapter3_Dec_20/Chapter3_Dec_20.Rmd",
    "properties" : {
        "last_setup_crc32" : "75472C79bb338d19",
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}