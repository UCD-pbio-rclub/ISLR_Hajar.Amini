{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Chapter6_Feb_20\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n#6.8 Exercises\nConceptual\n1. We perform best subset, forward stepwise, and backward stepwise\nselection on a single data set. For each approach, we obtain p + 1\nmodels, containing 0, 1, 2, . . . , p predictors. Explain your answers:\n(a) Which of the three models with k predictors has the smallest\ntraining RSS?\nBest subset selection has the smallest training RSS because the other two methods determine models based on which predictors they pick first. \n\n(b) Which of the three models with k predictors has the smallest\ntest RSS?\nBest subset selection have the smallest test RSS because it considers more models than the other methods, but the other models might can pick more model that fits the test data better.\n\n(c) True or False:\ni. The predictors in the k-variable model identified by forward\nstepwise are a subset of the predictors in the (k+1)-variable\nmodel identified by forward stepwise selection.\nTrue\nii. The predictors in the k-variable model identified by backward\nstepwise are a subset of the predictors in the (k + 1)-\nvariable model identified by backward stepwise selection.\nTrue\niii. The predictors in the k-variable model identified by backward\nstepwise are a subset of the predictors in the (k + 1)-\nvariable model identified by forward stepwise selection.\nFalse\niv. The predictors in the k-variable model identified by forward\nstepwise are a subset of the predictors in the (k+1)-variable\nmodel identified by backward stepwise selection.\nFalse\nv. The predictors in the k-variable model identified by best\nsubset are a subset of the predictors in the (k + 1)-variable\nmodel identified by best subset selection.\nFalse\n\n8. In this exercise, we will generate simulated data, and will then use\nthis data to perform best subset selection.\n\n(a) Use the rnorm() function to generate a predictor X of length\nn = 100, as well as a noise vector e of length n = 100.\n\n```{r}\nset.seed(1)\nX <- rnorm(100)\ne <- rnorm(100)\n```\n\n(b) Generate a response vector Y of length n = 100 according to\nthe model\nY = β0 + β1X + β2X2 + β3X3 + e,\nwhere β0, β1, β2, and β3 are constants of your choice.\n\n```{r}\nβ0 = 3 \n\nβ1 = 2 \nβ2 = -3 \nβ3=0.3\nY = β0 + β1 * X + β2 * X^2 + β3 * X^3 + e\n```\n\n(c) Use the regsubsets() function to perform best subset selection\nin order to choose the best model containing the predictors\nX,X2, . . .,X10. What is the best model obtained according to\nCp, BIC, and adjusted R2? Show some plots to provide evidence\nfor your answer, and report the coefficients of the best model obtained.\nNote you will need to use the data.frame() function to\ncreate a single data set containing both X and Y .\n\n```{r}\n#best modle having polynomial of X of degree 10\n#install.packages(\"leaps\")\nlibrary(leaps)\ndata <- data.frame(\"y\" = Y, \"x\" = X)\nmode <- regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10)\nmode.summary = summary(mode)\n# Find the model size for best cp, BIC and adjr2\nwhich.min(mode.summary$cp) #[1] 3\nwhich.min(mode.summary$bic) #[1] 3\nwhich.max(mode.summary$adjr2) #[1] 5\n# Plot cp, BIC and adjr2\nplot(mode.summary$cp, xlab=\"Subset degree\", ylab=\"Cp\", pch=20, type=\"l\")\npoints(3, mode.summary$cp[3], pch=4, col=\"red\", lwd=7)\nplot(mode.summary$bic, xlab=\"Subset degree\", ylab=\"BIC\", pch=20, type=\"l\")\npoints(3, mode.summary$bic[3], pch=4, col=\"red\", lwd=7)\nplot(mode.summary$adjr2, xlab=\"Subset degree\", ylab=\"Adjusted R2\", pch=20, type=\"l\")\npoints(3, mode.summary$adjr2[3], pch=4, col=\"red\", lwd=7)\n#We find that with Cp, BIC and Adjusted R2 criteria, 3, 3, and 3 variable models are respectively picked.\ncoefficients(mode, id=3)\n#All statistics pick $X^7$ over $X^3$. The remaining coefficients are quite close to $\\beta$ s.\n```\n\n(d) Repeat (c), using forward stepwise selection and also using backwards\nstepwise selection. How does your answer compare to the\nresults in (c)?\n\n```{r}\n#We fit forward and backward stepwise models to the data.\nmode.fwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method=\"forward\")\nmode.bwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method=\"backward\")\nfwd.summary = summary(mode.fwd)\nbwd.summary = summary(mode.bwd)\nwhich.min(fwd.summary$cp)\nwhich.min(bwd.summary$cp)\nwhich.min(fwd.summary$bic)\nwhich.min(bwd.summary$bic)\nwhich.max(fwd.summary$adjr2)\nwhich.max(bwd.summary$adjr2)\n# Plot the statistics\npar(mfrow=c(3, 2))\nplot(fwd.summary$cp, xlab=\"Subset Size\", ylab=\"Forward Cp\", pch=20, type=\"l\")\npoints(3, fwd.summary$cp[3], pch=4, col=\"red\", lwd=7)\nplot(bwd.summary$cp, xlab=\"Subset Size\", ylab=\"Backward Cp\", pch=20, type=\"l\")\npoints(3, bwd.summary$cp[3], pch=4, col=\"red\", lwd=7)\nplot(fwd.summary$bic, xlab=\"Subset Size\", ylab=\"Forward BIC\", pch=20, type=\"l\")\npoints(3, fwd.summary$bic[3], pch=4, col=\"red\", lwd=7)\nplot(bwd.summary$bic, xlab=\"Subset Size\", ylab=\"Backward BIC\", pch=20, type=\"l\")\npoints(3, bwd.summary$bic[3], pch=4, col=\"red\", lwd=7)\nplot(fwd.summary$adjr2, xlab=\"Subset Size\", ylab=\"Forward Adjusted R2\", pch=20, type=\"l\")\npoints(3, fwd.summary$adjr2[3], pch=4, col=\"red\", lwd=7)\nplot(bwd.summary$adjr2, xlab=\"Subset Size\", ylab=\"Backward Adjusted R2\", pch=20, type=\"l\")\npoints(4, bwd.summary$adjr2[4], pch=4, col=\"red\", lwd=7)\n#see that all statistics pick 3 variable models except backward stepwise with adjusted R2. Here are the coefficients\n\ncoefficients(mode.fwd, id=3)\ncoefficients(mode.bwd, id=3)\ncoefficients(mode.fwd, id=4)\n#Here forward stepwise picks $X^7$ over $X^3$. Backward stepwise with $3$ variables picks $X^9$ while backward stepwise with $4$ variables picks $X^4$ and $X^7$. All other coefficients are close to $\\beta$ s.\n```\n\n(e) Now fit a lasso model to the simulated data, again using X,X2,\n. . . , X10 as predictors. Use cross-validation to select the optimal\nvalue of λ. Create plots of the cross-validation error as a function\nof λ. Report the resulting coefficient estimates, and discuss the\nresults obtained.\n(f) Now generate a response vector Y according to the model\nY = β0 + β7X7 + ,\nand perform best subset selection and the lasso. Discuss the\nresults obtained.",
    "created" : 1519082921904.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "700099217",
    "id" : "F211715A",
    "lastKnownWriteTime" : 1519150942,
    "last_content_update" : 1519150942085,
    "path" : "~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter6_Feb_20/Chapter6_Feb_20.Rmd",
    "project_path" : "Chapter6_Feb_20/Chapter6_Feb_20.Rmd",
    "properties" : {
        "last_setup_crc32" : "43BC3185bb338d19",
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}