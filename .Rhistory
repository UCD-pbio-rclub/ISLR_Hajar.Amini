library(ISLR)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.fit = lda(Direction ~ Lag2, data =  Weekly)
lda.fit = lda(Direction ~ Lag2, data =  Weekly, subset=train)
lda.fit <- lda(Direction~lag2, data = Weekly,subet=train)
lda.fit <- lda(Direction~ lag2, data = Weekly,subet=train)
lda.fit <- lda(Direction ~ lag2, data = Weekly,subet=train)
names(Smarket )
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
train = (Year < 2009)
summary(Weekly)
train = (Year < 2009)
data(Weekly)
train = (Year < 2009)
train = (Weekly$Year < 2009)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.pred = predict(lda.fit, Weekly.0910)
setwd("~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter5_Feb_6")
knitr::opts_chunk$set(echo = TRUE)
library (ISLR)
set.seed (1)
train=sample (392 ,196)
?sample
lm.fit =lm(mpg∼horsepower ,data=Auto ,subset =train )
lm.fit =lm(mpg ∼ horsepower ,data=Auto ,subset =train )
lm.fit =lm (mpg ∼ horsepower ,data=Auto ,subset =train )
lm.fit =lm (mpg ∼ horsepower ,data=Auto , subset =train )
lm.fit =lm (mpg ∼ horsepower ,data=Auto , subset =train)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
attach (Auto)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg∼poly(horsepower ,2) ,data=Auto ,subset =train )
lm.fit2=lm(mpg~poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
lm.fit3=lm(mpg~poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
set.seed (2)
train=sample (392 ,196)
lm.fit =lm(mpg~horsepower ,subset =train)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg~poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
lm.fit3=lm(mpg~poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
glm.fit=glm(mpg~horsepower ,data=Auto)
coef(glm.fit)
lm.fit =lm(mpg∼horsepower ,data=Auto)
lm.fit =lm(mpg~horsepower ,data=Auto)
coef(lm.fit)
library (boot)
glm.fit=glm(mpg~horsepower ,data=Auto)
cv.err =cv.glm(Auto ,glm.fit)
cv.err$delta
cv.err =cv.glm(Auto ,glm.fit)
cv.err$delta
?cv.glm()
cv.err
head(cv.err)
head(cv.err$delta)
cv.error=rep (0,5)
cv.error
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm.fit)$delta [1]
}
cv.error
set.seed (17)
cv.error .10= rep (0 ,10)
cv.error.10= rep (0 ,10)
cv.error.10[1]
cv.error.10[2]
cv.error.10[10]
set.seed (17)
cv.error.10= rep (0 ,10)
for (i in 1:10) {
glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
set.seed (17)
cv.error.10= rep (0 ,10)
for (i in 1:10) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error.10[i]=cv.glm (Auto ,glm.fit ,K=10) $delta [1]
}
cv.error.10
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
IRLR
Default
summary(Default)
attach(Default)
set.seed(1)
binomial
?family
set.seed(1)
glm.fit = glm(default~income+balance, data=Default, family=binomial)
x = function() {
# i.
train = sample(dim(Default)[1], dim(Default)[1]/2)
# ii.
glm.fit = glm(default~income+balance, data=Default, family=binomial,
subset=train)
# iii.
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train,], type="response")
glm.pred[glm.probs>.5] = "Yes"
# iv.
return(mean(glm.pred != Default[-train,]$default))
}
x()
x()
x()
x()
train = sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit = glm(default~income+balance+student, data=Default, family=binomial,subset=train)
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train,], type="response")
glm.pred[glm.probs>.5] = "Yes"
mean(glm.pred != Default[-train,]$default)
library(ISLR)
summary(Weekly)
set.seed(1)
attach(Weekly)
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly, family=binomial)
summary(glm.fit)
Weekly[,]
Weekly[-1,]
rownames(Weekly)
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-1,], family=binomial)
summary(glm.fit)
predict.glm(glm.fit, Weekly[1,], type="response") > 0.5
Weekly[1,]
count = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-i,], family=binomial)
is_up = predict.glm(glm.fit, Weekly[i,], type="response") > 0.5
is_true_up = Weekly[i,]$Direction == "Up"
if (is_up != is_true_up)
count[i] = 1
}
sum(count)
y = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-i,], family=binomial)
is_up = predict.glm(glm.fit, Weekly[i,], type="response") > 0.5
is_true_up = Weekly[i,]$Direction == "Up"
if (is_up != is_true_up)
count[i] = 1
}
sum(y)
#490 errors.
mean(y)
mean(y)
sum(y)
y = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-i,], family=binomial)
is_up = predict.glm(glm.fit, Weekly[i,], type="response") > 0.5
is_true_up = Weekly[i,]$Direction == "Up"
if (is_up != is_true_up)
y[i] = 1
}
sum(y)
#490 errors.
mean(y)
set .seed (1)
set.seed (1)
x=rnorm (100)
y=x-2* x^2+ rnorm (100)
plot(x,y)
library(boot)
Data = data.frame(x,y)
set.seed(1)
glm.fit = glm(y~x)
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,2))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,3))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,4))
cv.glm(Data, glm.fit)$delta
set.seed(10)
glm.fit = glm(y~x)
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,2))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,3))
glm.fit = glm(y~poly(x,3))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,4))
cv.glm(Data, glm.fit)$delta
summary(glm.fit)
print
setwd("~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter5_Feb_13")
knitr::opts_chunk$set(echo = TRUE)
alpha.fn=function (data ,index){
X=data$X [index]
Y=data$Y [index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
data
X
Y
Portfolio
data(Portfolio)
alpha.fn(Portfolio ,1:100)
library (ISLR)
alpha.fn(Portfolio ,1:100)
data(Portfolio)
Portfolio
set.seed (1)
alpha.fn(Portfolio ,sample (100 ,100 , replace =T))
boot(Portfolio ,alpha.fn,R=1000)
library (boot)
boot(Portfolio ,alpha.fn,R=1000)
boot(data = Portfolio , statistic = alpha.fn, R = 1000)
boot.fn=function (data ,index )
+ return (coef(lm(mpg∼horsepower ,data=data ,subset =index)))
+ return (coef(lm(mpg~horsepower ,data=data ,subset =index)))
boot.fn=function (data ,index )
+ return (coef(lm(mpg~horsepower ,data=data ,subset =index)))
boot.fn(Auto ,1:392)
set.seed (1)
boot.fn(Auto ,sample (392 ,392 , replace =T))
boot.fn(Auto ,sample (392 ,392 , replace =T))
boot(Auto ,boot.fn ,1000)
summary (lm(mpg~horsepower ,data=Auto))$coef
summary (lm(mpg~horsepower ,data=Auto))
summary (lm(mpg~horsepower ,data=Auto))$coef
boot.fn=function (data ,index )
+ coefficients(lm(mpg∼horsepower +I( horsepower ^2) ,data=data ,
+ coefficients(lm(mpg~horsepower +I( horsepower ^2) ,data=data ,
subset =index))
+ coefficients(lm(mpg~horsepower +I(horsepower ^2) ,data=data ,
subset =index))
set.seed (1)
boot(Auto ,boot.fn ,1000)
boot(data = Auto , statistic = boot.fn, R = 1000)
summary (lm(mpg∼horsepower +I(horsepower ^2) ,data=Auto))$coef
summary (lm(mpg~horsepower +I(horsepower ^2) ,data=Auto))$coef
boot.fn=function (data ,index)
+ coefficients(lm(mpg~horsepower +I(horsepower ^2) ,data=data ,
subset =index))
library(ISLR)
summary(Default)
attach(Default)
set.seed(1)
glm.fit = glm(default~income+balance, data=Default, family=binomial)
summary(glm.fit)
boot.fn = function(data, index)
return(coef(glm(default~income+balance, data=data, family=binomial,
subset=index)))
boot.fn
library(boot)
boot(Default, boot.fn, 50)
boot.fn = function(data, index)
return(coef(glm(default~income+balance, data=data, family=binomial,
subset=index)))
library(boot)
boot(Default, boot.fn, 50)
library(MASS)
summary(Boston)
set.seed(1)
medv_mean = mean(medv)
attach(Boston)
medv_mean = mean(medv)
medv_mean
length(medv)
medv
medv_err = sd(medv) / sqrt(length(medv))
medv.err
medv_err
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bootstrap = boot(medv, boot.fn, 1000)
bootstrap
medv_err = sd(medv) / sqrt(length(medv))
medv_err
medv %>% head()
head(medv)
Boston
t.test(medv)
bootstrap
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bootstrap = boot(medv, boot.fn, 1000)
bootstrap
#0.4119374
#Similar to answer from (b)(0.4119 vs 0.4089)
bootstrap$t0
bootstrap$t
bootstrap$t1
t.test(medv)
c(bootstrap$t0 - 2*0.4119, bootstrap$t0 + 2*0.4119)
t.test(medv)
median(medv)
boot.fn = function(data, index) return(median(data[index]))
boot(medv, boot.fn, 1000)
medv_tenth = quantile(medv, c(0.1))
medv_tenth
boot.fn = function(data, index) return(quantile(data[index], c(0.1)))
boot(medv, boot.fn, 1000)
x = 1:100000
pr = function(n) return(1 - (1 - 1/n)^n)
plot(x, pr(x))
knitr::opts_chunk$set(echo = TRUE)
store=rep (NA , 10000)
for (i in 1:10000) {
store[i]=sum(sample (1:100 , rep =TRUE)==4) >0
}
mean(store)
set.seed(1)
store = rep(NA, 1e4)
for (i in 1:1e4) {
store[i] = sum(sample(1:100, rep=T) == 4) > 0
}
mean(store)
set.seed(1)
store = rep(NA, 10000)
for (i in 1:10000) {
store[i] = sum(sample(1:100, rep=T) == 4) > 0
}
mean(store)
The numerical results show an  mean probability of 64.1%, close to our theoretically result
set.seed(1)
store = rep(NA, 10000)
for (i in 1:10000) {
store[i] = sum(sample(1:100, rep=T) == 4) > 0
}
mean(store)
#The numerical results show an  mean probability of 64.1%, close to our theoretically result
sd
sd(1)
sd(2)
cov()
cov
sd
function(2.na.rm = FALSE)
function(x,na.rm = FALSE)
)
x = 2
sd
sd(c)
sd(x)
joinleft
1-1/n
n=1
(1-1/n)^n
sd(n)
sd((1-1/n)^n)
function()
)
function\
function
>
alpha.fn(100)
alpha.fn <- function(data, indesx)
{x=data(y)}
alpha.fn()
View(alpha.fn)
View(boot.fn)
rm(alpha.fn())
rm(alpha.fn
)
library (ISLR)
alpha.fn=function (data ,index){
X=data$X [index]
Y=data$Y [index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
setwd("~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter6_Feb_20")
knitr::opts_chunk$set(echo = TRUE)
set.seed(1)
X <- nrow(100)
e <- rnorm(100)
X <- rnorm(100)
rm(x)
library(leaps)
install.packages(leaps)
library(ISLR)
library(leaps)
install.packages("leaps")
library(leaps)
data <- data.frame("y" = Y, "x" = X)
library(leaps)
data <- data.frame("y" = Y, "x" = X)
data.full <- data.frame("y" = Y, "x" = X)
β0 = 3 , β1 = 2 , , β2 = -3 , β3=0.3
Y = β0 + β1 * X + β2 * X^2 + β3 * X^3 + e
Y = β0 + β1X + β2X2 + β3X3 + e
mod.full = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10)
data <- data.frame("y" = Y, "x" = X)
data <- data.frame("y" = Y)
β0 = 3
β1 = 2
β2 = -3
β3=0.3
Y = β0 + β1 * X + β2 * X^2 + β3 * X^3
Y = β0 + β1 * X + β2 * X^2 + β3 * X^3 + e
data <- data.frame("y" = Y, "x" = X)
data
?regsubsets
mode <- regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10)
mode
mode.summary = summary(mode)
mode.summary
which.min(mode.summary$cp)
which.min(mode.summary$bic)
which.min(mode.summary$bic)
which.max(mode.summary$adjr2)
plot(mode.summary$cp, xlab="Subset Size", ylab="Cp", pch=20, type="l")
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
plot(mod.summary$bic, xlab="Subset degree", ylab="BIC", pch=20, type="l")
plot(mode.summary$bic, xlab="Subset degree", ylab="BIC", pch=20, type="l")
plot(mode.summary$adjr2, xlab="Subset degree", ylab="Adjusted R2", pch=20, type="l")
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
plot(mode.summary$cp, xlab="Subset degree", ylab="Cp", pch=20, type="l")
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
points(3, mod.summary$bic[3], pch=4, col="red", lwd=7)
plot(mode.summary$adjr2, xlab="Subset degree", ylab="Adjusted R2", pch=20, type="l")
points(3, mod.summary$adjr2[3], pch=4, col="red", lwd=7)
plot(mode.summary$adjr2, xlab="Subset degree", ylab="Adjusted R2", pch=20, type="l")
points(3, mode.summary$adjr2[3], pch=4, col="red", lwd=7)
coefficients(mode, id=3)
mode.fwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="forward")
mode.bwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="backward")
fwd.summary = summary(mode.fwd)
bwd.summary = summary(mode.bwd)
which.min(fwd.summary$cp)
which.min(bwd.summary$cp)
which.min(fwd.summary$bic)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
which.max(bwd.summary$adjr2)
par(mfrow=c(3, 2))
plot(fwd.summary$cp, xlab="Subset Size", ylab="Forward Cp", pch=20, type="l")
points(3, fwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(fwd.summary$cp, xlab="Subset Size", ylab="Forward Cp", pch=20, type="l")
points(3, fwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(bwd.summary$cp, xlab="Subset Size", ylab="Backward Cp", pch=20, type="l")
points(3, bwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(fwd.summary$bic, xlab="Subset Size", ylab="Forward BIC", pch=20, type="l")
points(3, fwd.summary$bic[3], pch=4, col="red", lwd=7)
plot(bwd.summary$bic, xlab="Subset Size", ylab="Backward BIC", pch=20, type="l")
points(3, bwd.summary$bic[3], pch=4, col="red", lwd=7)
plot(fwd.summary$adjr2, xlab="Subset Size", ylab="Forward Adjusted R2", pch=20, type="l")
points(3, fwd.summary$adjr2[3], pch=4, col="red", lwd=7)
plot(bwd.summary$adjr2, xlab="Subset Size", ylab="Backward Adjusted R2", pch=20, type="l")
points(4, bwd.summary$adjr2[4], pch=4, col="red", lwd=7)
coefficients(mode, id=3)
best modle having polynomial of X of degree 10
#best modle having polynomial of X of degree 10
install.packages("leaps")
library(leaps)
data <- data.frame("y" = Y, "x" = X)
mode <- regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10)
mode.summary = summary(mode)
# Find the model size for best cp, BIC and adjr2
which.min(mode.summary$cp) #[1] 3
which.min(mode.summary$bic) #[1] 3
which.max(mode.summary$adjr2) #[1] 5
# Plot cp, BIC and adjr2
plot(mode.summary$cp, xlab="Subset degree", ylab="Cp", pch=20, type="l")
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
plot(mode.summary$bic, xlab="Subset degree", ylab="BIC", pch=20, type="l")
points(3, mod.summary$bic[3], pch=4, col="red", lwd=7)
install.packages("leaps")
which.min(mode.summary$cp) #[1] 3
which.min(mode.summary$bic) #[1] 3
which.max(mode.summary$adjr2) #[1] 5
# Plot cp, BIC and adjr2
plot(mode.summary$cp, xlab="Subset degree", ylab="Cp", pch=20, type="l")
points(3, mode.summary$cp[3], pch=4, col="red", lwd=7)
plot(mode.summary$bic, xlab="Subset degree", ylab="BIC", pch=20, type="l")
points(3, mode.summary$bic[3], pch=4, col="red", lwd=7)
plot(mode.summary$adjr2, xlab="Subset degree", ylab="Adjusted R2", pch=20, type="l")
points(3, mode.summary$adjr2[3], pch=4, col="red", lwd=7)
#We find that with Cp, BIC and Adjusted R2 criteria, 3, 3, and 3 variable models are respectively picked.
coefficients(mode, id=3)
#We fit forward and backward stepwise models to the data.
mode.fwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="forward")
mode.bwd = regsubsets(y~poly(x, 10, raw=T), data=data, nvmax=10, method="backward")
fwd.summary = summary(mode.fwd)
bwd.summary = summary(mode.bwd)
which.min(fwd.summary$cp)
which.min(bwd.summary$cp)
which.min(fwd.summary$bic)
which.min(bwd.summary$bic)
which.max(fwd.summary$adjr2)
which.max(bwd.summary$adjr2)
# Plot the statistics
par(mfrow=c(3, 2))
plot(fwd.summary$cp, xlab="Subset Size", ylab="Forward Cp", pch=20, type="l")
points(3, fwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(bwd.summary$cp, xlab="Subset Size", ylab="Backward Cp", pch=20, type="l")
points(3, bwd.summary$cp[3], pch=4, col="red", lwd=7)
plot(fwd.summary$bic, xlab="Subset Size", ylab="Forward BIC", pch=20, type="l")
points(3, fwd.summary$bic[3], pch=4, col="red", lwd=7)
plot(bwd.summary$bic, xlab="Subset Size", ylab="Backward BIC", pch=20, type="l")
points(3, bwd.summary$bic[3], pch=4, col="red", lwd=7)
plot(fwd.summary$adjr2, xlab="Subset Size", ylab="Forward Adjusted R2", pch=20, type="l")
points(3, fwd.summary$adjr2[3], pch=4, col="red", lwd=7)
plot(bwd.summary$adjr2, xlab="Subset Size", ylab="Backward Adjusted R2", pch=20, type="l")
points(4, bwd.summary$adjr2[4], pch=4, col="red", lwd=7)
#see that all statistics pick 3 variable models except backward stepwise with adjusted R2. Here are the coefficients
coefficients(mod.fwd, id=3)
