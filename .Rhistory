library (MASS)
library (ISLR)
data(Auto)
library (MASS)
library (ISLR)
data(Auto)
lm.fit1 = lm(mpg~.-name, data=Auto)
summary(lm.fit1)
par(mfrow=c(2,2))
plot(lm.fit1)
#The fit does not appear to be accurate because there is an obvious curve pattern to the residuals plots. From the leverage plot, point 14 appears to have high leverage, without a high magnitude residual.
plot(predict(lm.fit1), rstudent(lm.fit1))
#There are possible outliers regarding  plot of studentized residuals because there are data with a value greater than 3.
lm.fit =lm(medv∼lstat+age ,data=Boston )
lm.fit =lm(medv∼lstat+age,data=Boston )
Boston
lm.fit =lm(medv∼lstat+age,data=Boston )
library (MASS)
library (ISLR)
lm.fit =lm(medv∼lstat+age,data=Boston )
lm.fit = lm(medv∼lstat+age,data=Boston )
lm.fit <- lm(data = Boston, medv~lstat+age)
summary(lm.fit)
lm.fit <- lm(medv~. , data=Boston)
summary(lm.fit)
summary(lm.fit)$r.sq
summary(lm.fit)$sigma
library (car)
install.packages(car)
install.packages("car")
library (car)
vif(lm.fit)
lm.fit1=lm(medv∼.-age ,data=Boston )
lm.fit1 >- lm(medv~.-age, data = Boston)
lm.fit1 >- lm(medv~. -age, data = Boston)
lm.fit1=lm(medv~. -age, data = Boston)
lm.fit1=lm(medv~.-age, data = Boston)
summary (lm.fit1)
lm.fit1=update (lm.fit , ∼.-age)
lm.fit1=update(lm.fit , ∼.-age)
lm.fit1=update(lm.fit, ∼.-age)
lm.fit1=update(lm.fit,∼.-age)
update()
lm.fit1=update (lm.fit,∼. -age)
lm.fit1= update (lm.fit,∼. -age)
lm.fit1=update(lm.fit, ~. -age)
lm.fit1
summary (lm(medv∼lstat *age ,data=Boston ))
summary(lm(medv~lstat *age, data = Boston))
lm.fit2=lm(medv∼lstat +I(lstat ^2))
lm.fit2=lm(medv~lstat + I(lstat^2))
lm.fit2=lm (medv~lstat + I(lstat^2))
lm.fit2=lm(medv~ lstat + I(lstat^2))
lm.fit2=lm(medv~lstat +I(lstat^2))
lm.fit2=lm(medv~lstat +I(lstat^2), data = Boston)
summary(lm.fit2)
lm.fit =lm(medv∼lstat, data=Boston)
lm.fit=lm(medv∼lstat, data=Boston)
lm.fit=lm(medv~lstat, data=Boston)
summary(lm.fit)
anova(lm.fit ,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5=lm(medv∼poly(lstat ,5), data=Boston)
lm.fit5=lm(medv∼poly(lstat,5), data=Boston)
lm.fit5=lm(medv~ploy(lstat, 5), data = Boston)
lm.fit5=lm(medv~ ploy(lstat, 5), data = Boston)
lm.fit5=lm(medv~ poly(lstat, 5), data = Boston)
summary(lm.fit5)
summary (lm(medv∼log(rm),data=Boston ))
summary(lm(medv~log(rm), data = Boston))
fix( Carseats )
?fix
names(Carseats )
lm.fit =lm(Sales∼.+ Income :Advertising +Price :Age ,data=Carseats )
lm.fit=lm(Sales~.+Income:Advertising+Price:Age, data=Carseats)
summary (lm.fit)
attach (Carseats )
contrasts (ShelveLoc )
Carseats$
>
?attach
?contrasts
ShelveLocGood
contrasts (ShelveLoc )
Carseats$ShelveLoc
knitr::opts_chunk$set(echo = TRUE)
set.seed (1)
x1=runif (100)
x2 =0.5* x1+rnorm (100) /10
y=2+2* x1 +0.3* x2+rnorm (100)
Y = 2 + 2 X_1 + 0.3 X_2 + \epsilon \ \beta_0 = 2, \beta_1 = 2, \beta_3 = 0.3
y
cor(x1, x2)
plot(x1, x2)
lm.fit = lm(y~x1+x2)
summary(lm.fit)
lm.fit = lm(y~x1)
summary(lm.fit)
lm.fit = lm(y~x2)
summary(lm.fit)
summary (lm(medv ∼ lstat *age ,data=Boston ))
summary(lm(medv~lstat*age, data = Boston))
x1=c(x1 , 0.1)
x2=c(x2 , 0.8)
y=c(y,6)
lm.fit1 = lm(y~x1+x2)
summary(lm.fit1)
lm.fit2 = lm(y~x1)
summary(lm.fit2)
lm.fit3 = lm(y~x2)
summary(lm.fit3)
par(mfrow=c(2,2))
plot(lm.fit1)
par(mfrow=c(2,2))
plot(lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit3)
plot(predict(lm.fit1), rstudent(lm.fit1))
plot(predict(lm.fit2), rstudent(lm.fit2))
plot(predict(lm.fit3), rstudent(lm.fit3))
plot(predict(lm.fit1), rstudent(lm.fit1))
plot(predict(lm.fit2), rstudent(lm.fit2))
load("/Users/hajaramini/Downloads/metaAnalysisFiles/metaAnalysisData.RData")
View(datExprA1)
View(datExprB2)
View(datExprB1)
View(datExprA2)
View(datExprB2)
View(datExprB1)
getwd()
setwd("Chapter4_Jan_23/")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
data(Weekly)
Weekly
library(ISLR)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.fit = lda(Direction ~ Lag2, data =  Weekly)
lda.fit = lda(Direction ~ Lag2, data =  Weekly, subset=train)
lda.fit <- lda(Direction~lag2, data = Weekly,subet=train)
lda.fit <- lda(Direction~ lag2, data = Weekly,subet=train)
lda.fit <- lda(Direction ~ lag2, data = Weekly,subet=train)
names(Smarket )
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
train = (Year < 2009)
summary(Weekly)
train = (Year < 2009)
data(Weekly)
train = (Year < 2009)
train = (Weekly$Year < 2009)
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = train)
lda.pred = predict(lda.fit, Weekly.0910)
setwd("~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter5_Feb_6")
knitr::opts_chunk$set(echo = TRUE)
library (ISLR)
set.seed (1)
train=sample (392 ,196)
?sample
lm.fit =lm(mpg∼horsepower ,data=Auto ,subset =train )
lm.fit =lm(mpg ∼ horsepower ,data=Auto ,subset =train )
lm.fit =lm (mpg ∼ horsepower ,data=Auto ,subset =train )
lm.fit =lm (mpg ∼ horsepower ,data=Auto , subset =train )
lm.fit =lm (mpg ∼ horsepower ,data=Auto , subset =train)
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
attach (Auto)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg∼poly(horsepower ,2) ,data=Auto ,subset =train )
lm.fit2=lm(mpg~poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
lm.fit3=lm(mpg~poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
set.seed (2)
train=sample (392 ,196)
lm.fit =lm(mpg~horsepower ,subset =train)
mean((mpg -predict (lm.fit ,Auto))[-train ]^2)
lm.fit2=lm(mpg~poly(horsepower ,2) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit2 ,Auto))[-train ]^2)
lm.fit3=lm(mpg~poly(horsepower ,3) ,data=Auto ,subset =train )
mean((mpg -predict (lm.fit3 ,Auto))[-train ]^2)
glm.fit=glm(mpg~horsepower ,data=Auto)
coef(glm.fit)
lm.fit =lm(mpg∼horsepower ,data=Auto)
lm.fit =lm(mpg~horsepower ,data=Auto)
coef(lm.fit)
library (boot)
glm.fit=glm(mpg~horsepower ,data=Auto)
cv.err =cv.glm(Auto ,glm.fit)
cv.err$delta
cv.err =cv.glm(Auto ,glm.fit)
cv.err$delta
?cv.glm()
cv.err
head(cv.err)
head(cv.err$delta)
cv.error=rep (0,5)
cv.error
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm .fit)$delta [1]
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error[i]=cv.glm (Auto ,glm.fit)$delta [1]
}
cv.error
set.seed (17)
cv.error .10= rep (0 ,10)
cv.error.10= rep (0 ,10)
cv.error.10[1]
cv.error.10[2]
cv.error.10[10]
set.seed (17)
cv.error.10= rep (0 ,10)
for (i in 1:10) {
glm.fit=glm(mpg∼poly(horsepower ,i),data=Auto)
set.seed (17)
cv.error.10= rep (0 ,10)
for (i in 1:10) {
glm.fit=glm(mpg~poly(horsepower ,i),data=Auto)
cv.error.10[i]=cv.glm (Auto ,glm.fit ,K=10) $delta [1]
}
cv.error.10
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
IRLR
Default
summary(Default)
attach(Default)
set.seed(1)
binomial
?family
set.seed(1)
glm.fit = glm(default~income+balance, data=Default, family=binomial)
x = function() {
# i.
train = sample(dim(Default)[1], dim(Default)[1]/2)
# ii.
glm.fit = glm(default~income+balance, data=Default, family=binomial,
subset=train)
# iii.
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train,], type="response")
glm.pred[glm.probs>.5] = "Yes"
# iv.
return(mean(glm.pred != Default[-train,]$default))
}
x()
x()
x()
x()
train = sample(dim(Default)[1], dim(Default)[1]/2)
glm.fit = glm(default~income+balance+student, data=Default, family=binomial,subset=train)
glm.pred = rep("No", dim(Default)[1]/2)
glm.probs = predict(glm.fit, Default[-train,], type="response")
glm.pred[glm.probs>.5] = "Yes"
mean(glm.pred != Default[-train,]$default)
library(ISLR)
summary(Weekly)
set.seed(1)
attach(Weekly)
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly, family=binomial)
summary(glm.fit)
Weekly[,]
Weekly[-1,]
rownames(Weekly)
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-1,], family=binomial)
summary(glm.fit)
predict.glm(glm.fit, Weekly[1,], type="response") > 0.5
Weekly[1,]
count = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-i,], family=binomial)
is_up = predict.glm(glm.fit, Weekly[i,], type="response") > 0.5
is_true_up = Weekly[i,]$Direction == "Up"
if (is_up != is_true_up)
count[i] = 1
}
sum(count)
y = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-i,], family=binomial)
is_up = predict.glm(glm.fit, Weekly[i,], type="response") > 0.5
is_true_up = Weekly[i,]$Direction == "Up"
if (is_up != is_true_up)
count[i] = 1
}
sum(y)
#490 errors.
mean(y)
mean(y)
sum(y)
y = rep(0, dim(Weekly)[1])
for (i in 1:(dim(Weekly)[1])) {
glm.fit = glm(Direction~Lag1+Lag2, data=Weekly[-i,], family=binomial)
is_up = predict.glm(glm.fit, Weekly[i,], type="response") > 0.5
is_true_up = Weekly[i,]$Direction == "Up"
if (is_up != is_true_up)
y[i] = 1
}
sum(y)
#490 errors.
mean(y)
set .seed (1)
set.seed (1)
x=rnorm (100)
y=x-2* x^2+ rnorm (100)
plot(x,y)
library(boot)
Data = data.frame(x,y)
set.seed(1)
glm.fit = glm(y~x)
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,2))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,3))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,4))
cv.glm(Data, glm.fit)$delta
set.seed(10)
glm.fit = glm(y~x)
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,2))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,3))
glm.fit = glm(y~poly(x,3))
cv.glm(Data, glm.fit)$delta
glm.fit = glm(y~poly(x,4))
cv.glm(Data, glm.fit)$delta
summary(glm.fit)
print
setwd("~/Documents/UCD-pbio-rclub/ISLR_Hajar.Amini/Chapter5_Feb_13")
knitr::opts_chunk$set(echo = TRUE)
alpha.fn=function (data ,index){
X=data$X [index]
Y=data$Y [index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
data
X
Y
Portfolio
data(Portfolio)
alpha.fn(Portfolio ,1:100)
library (ISLR)
alpha.fn(Portfolio ,1:100)
data(Portfolio)
Portfolio
set.seed (1)
alpha.fn(Portfolio ,sample (100 ,100 , replace =T))
boot(Portfolio ,alpha.fn,R=1000)
library (boot)
boot(Portfolio ,alpha.fn,R=1000)
boot(data = Portfolio , statistic = alpha.fn, R = 1000)
boot.fn=function (data ,index )
+ return (coef(lm(mpg∼horsepower ,data=data ,subset =index)))
+ return (coef(lm(mpg~horsepower ,data=data ,subset =index)))
boot.fn=function (data ,index )
+ return (coef(lm(mpg~horsepower ,data=data ,subset =index)))
boot.fn(Auto ,1:392)
set.seed (1)
boot.fn(Auto ,sample (392 ,392 , replace =T))
boot.fn(Auto ,sample (392 ,392 , replace =T))
boot(Auto ,boot.fn ,1000)
summary (lm(mpg~horsepower ,data=Auto))$coef
summary (lm(mpg~horsepower ,data=Auto))
summary (lm(mpg~horsepower ,data=Auto))$coef
boot.fn=function (data ,index )
+ coefficients(lm(mpg∼horsepower +I( horsepower ^2) ,data=data ,
+ coefficients(lm(mpg~horsepower +I( horsepower ^2) ,data=data ,
subset =index))
+ coefficients(lm(mpg~horsepower +I(horsepower ^2) ,data=data ,
subset =index))
set.seed (1)
boot(Auto ,boot.fn ,1000)
boot(data = Auto , statistic = boot.fn, R = 1000)
summary (lm(mpg∼horsepower +I(horsepower ^2) ,data=Auto))$coef
summary (lm(mpg~horsepower +I(horsepower ^2) ,data=Auto))$coef
boot.fn=function (data ,index)
+ coefficients(lm(mpg~horsepower +I(horsepower ^2) ,data=data ,
subset =index))
library(ISLR)
summary(Default)
attach(Default)
set.seed(1)
glm.fit = glm(default~income+balance, data=Default, family=binomial)
summary(glm.fit)
boot.fn = function(data, index)
return(coef(glm(default~income+balance, data=data, family=binomial,
subset=index)))
boot.fn
library(boot)
boot(Default, boot.fn, 50)
boot.fn = function(data, index)
return(coef(glm(default~income+balance, data=data, family=binomial,
subset=index)))
library(boot)
boot(Default, boot.fn, 50)
library(MASS)
summary(Boston)
set.seed(1)
medv_mean = mean(medv)
attach(Boston)
medv_mean = mean(medv)
medv_mean
length(medv)
medv
medv_err = sd(medv) / sqrt(length(medv))
medv.err
medv_err
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bootstrap = boot(medv, boot.fn, 1000)
bootstrap
medv_err = sd(medv) / sqrt(length(medv))
medv_err
medv %>% head()
head(medv)
Boston
t.test(medv)
bootstrap
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bootstrap = boot(medv, boot.fn, 1000)
bootstrap
#0.4119374
#Similar to answer from (b)(0.4119 vs 0.4089)
bootstrap$t0
bootstrap$t
bootstrap$t1
t.test(medv)
c(bootstrap$t0 - 2*0.4119, bootstrap$t0 + 2*0.4119)
t.test(medv)
median(medv)
boot.fn = function(data, index) return(median(data[index]))
boot(medv, boot.fn, 1000)
medv_tenth = quantile(medv, c(0.1))
medv_tenth
boot.fn = function(data, index) return(quantile(data[index], c(0.1)))
boot(medv, boot.fn, 1000)
x = 1:100000
pr = function(n) return(1 - (1 - 1/n)^n)
plot(x, pr(x))
knitr::opts_chunk$set(echo = TRUE)
store=rep (NA , 10000)
for (i in 1:10000) {
store[i]=sum(sample (1:100 , rep =TRUE)==4) >0
}
mean(store)
set.seed(1)
store = rep(NA, 1e4)
for (i in 1:1e4) {
store[i] = sum(sample(1:100, rep=T) == 4) > 0
}
mean(store)
set.seed(1)
store = rep(NA, 10000)
for (i in 1:10000) {
store[i] = sum(sample(1:100, rep=T) == 4) > 0
}
mean(store)
The numerical results show an  mean probability of 64.1%, close to our theoretically result
set.seed(1)
store = rep(NA, 10000)
for (i in 1:10000) {
store[i] = sum(sample(1:100, rep=T) == 4) > 0
}
mean(store)
#The numerical results show an  mean probability of 64.1%, close to our theoretically result
sd
sd(1)
sd(2)
cov()
cov
sd
function(2.na.rm = FALSE)
function(x,na.rm = FALSE)
)
x = 2
sd
sd(c)
sd(x)
joinleft
1-1/n
n=1
(1-1/n)^n
sd(n)
sd((1-1/n)^n)
function()
)
function\
function
>
alpha.fn(100)
alpha.fn <- function(data, indesx)
{x=data(y)}
alpha.fn()
View(alpha.fn)
View(boot.fn)
rm(alpha.fn())
rm(alpha.fn
)
library (ISLR)
alpha.fn=function (data ,index){
X=data$X [index]
Y=data$Y [index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
